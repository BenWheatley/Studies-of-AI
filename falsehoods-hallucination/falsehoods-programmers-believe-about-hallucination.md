# 1. Falsehoods Regarding Data and Training

* AI models only hallucinate when trained on bad data.
* More data will solve hallucination problems.
* AI hallucinations are solely a result of biased training data.
* Hallucinations are always due to missing data in the training set.
* AI hallucinations can be fully corrected by retraining on more data.
* AI hallucinations are more likely in models trained on unsupervised data.
* AI models trained on a diverse dataset will never hallucinate.
* Hallucinations can be fully prevented by using larger datasets.
* Hallucinations are mainly due to overfitting.
* Hallucinations are simply a result of underfitting.
* AI models trained in supervised learning won’t hallucinate.
* AI models trained with reinforcement learning won’t hallucinate.
* AI models hallucinate because they "try too hard" to please the user.

# 2. Falsehoods Regarding Model Architecture and Performance

* Hallucinations are just like any other bugs.
* Hallucinations are purely a technical issue.
* Advanced AI models don’t hallucinate.
* AI models that use more compute resources are less likely to hallucinate.
* AI hallucinations indicate that the model is over-regularizing.
* AI hallucinations can be ignored as long as the model's overall accuracy is high.
* AI hallucinations are just glorified typographical errors.
* AI models hallucinate because they "get bored" or "run out of data."
* The hallucination problem is unique to language models and doesn’t affect other AI systems.
* AI models hallucinate more in low-resource languages.
* AI hallucinations will disappear with the next generation of AI models.
* AI hallucinations are merely a result of ambiguous inputs.
* AI hallucinations are just a product of models being too complex.
* AI hallucinations are not a concern for well-maintained systems.
* AI hallucinations are an irreversible and inherent flaw of all AI models.
* Hallucinations are a sign that AI models are fundamentally flawed and unreliable.
* AI hallucinations mean that the model is losing its ability to generalize.
* Hallucinations are always a sign that an AI model is fundamentally broken.
* The more parameters a model has, the less likely it is to hallucinate.
* Models that use more compute resources are less likely to hallucinate.

# 3. Falsehoods Regarding Hallucination Frequency and Impact

* AI hallucinations are inevitable in every interaction.
* AI models are constantly hallucinating.
* Every AI hallucination is catastrophic.
* Hallucinations will always involve obvious factual errors.
* Hallucinations are always obvious and easy to spot.
* Hallucinations only happen under extreme or edge-case scenarios.
* AI models will always produce the same hallucinations with the same input.
* AI hallucinations are purely random.
* AI hallucinations occur mainly in low-resource languages.
* Hallucinations will always be similar across different models.
* AI hallucinations are equivalent to human errors.
* AI hallucinations are just a side effect of generative AI and aren’t worth worrying about.
* AI hallucinations can be ignored as long as the model's overall accuracy is high.
* AI hallucinations are a minor inconvenience and don’t affect overall accuracy.
* AI hallucinations are a temporary problem that will soon be solved.
* Hallucinations will always involve obvious factual errors.

# 4. Falsehoods Regarding Application and Domain Specificity

* AI hallucinations will make AI systems unfit for any critical application.
* Only low-quality or open-source models hallucinate.
* AI hallucinations are mainly an issue with text generation, not in other domains.
* Hallucinations only occur with text-based models.
* AI hallucinations are purely a problem for generative AI.
* AI hallucinations are specific to natural language processing models.
* AI hallucinations are solely a problem when AI models interact with humans.
* AI models trained on specific domains won’t hallucinate outside that domain.
* AI hallucinations are primarily a problem in English and not in other languages.
* AI hallucinations are a temporary problem that will soon be solved.
* Hallucinations are only a problem when AI models interact with humans.

# 5. Falsehoods Regarding Model Trust and Reliability

* Once a model hallucinates, it can no longer be trusted.
* AI models that hallucinate are untrustworthy and should not be used.
* AI hallucinations are proof that AI models can’t be trusted in any context.
* AI systems that hallucinate more as they age or are used over time.
* AI hallucinations prove that AI models are fundamentally flawed and unreliable.
* AI hallucinations represent malicious intent or bias.
* AI hallucinations mean the model is unusable.
* AI hallucinations are a sign of sentience.
* AI hallucinations will make AI systems unfit for any critical application.
* AI models hallucinate more in low-resource languages.
* Once an AI model starts hallucinating, it will continue to do so indefinitely.

# 6. Falsehoods Regarding Mitigation and Management of Hallucinations

* Hallucinations can be completely eliminated with enough training.
* AI hallucinations are easily fixed by simply correcting the model.
* AI systems will learn to stop hallucinating on their own.
* AI hallucinations are unmanageable and cannot be predicted.
* Hallucinations will naturally decrease over time with model usage.
* AI hallucinations are purely random and can’t be mitigated.
* AI hallucinations are purely random.
* AI hallucinations are simply a matter of incorrect probabilities.
* AI hallucinations can be ignored as long as the model's overall accuracy is high.
* AI hallucinations can be easily fixed with a simple tweak.
* Fine-tuning models for specific domains eliminates hallucinations.
* Hallucinations can be fully prevented by using larger datasets.
* AI hallucinations are predictable and can be easily mitigated.
* AI hallucinations can be fully prevented by using larger datasets.
* Hallucinations will naturally decrease over time with model usage.
* AI hallucinations are purely a matter of incorrect probabilities.

# 7. Falsehoods Regarding Model Usage and Development

* AI hallucinations are always obvious and easy to spot.
* AI hallucinations will always involve obvious factual errors.
* Hallucinations prove that AI models are fundamentally flawed and unreliable.
* AI models hallucinate because they "get bored" or "run out of data."
* AI models that hallucinate need to be discarded.
* AI hallucinations mean that AI models are not mature enough for use.
* Hallucinations indicate that AI models are losing their ability to generalize.
* AI hallucinations mean that AI models are not mature enough for use.
* AI models that hallucinate need to be discarded.
* Hallucinations are just a temporary problem that will soon be solved.
* AI models hallucinate because they "get bored" or "run out of data."
* AI hallucinations mean that AI models are not mature enough for use.

# 8. Falsehoods Regarding Ethical, Legal, and Social Considerations

* AI hallucinations represent malicious intent or bias.
* AI hallucinations prove that AI models can’t be trusted in any context.
* AI hallucinations are just a side effect of generative AI and aren’t worth worrying about.
* AI hallucinations indicate that the model is over-regularizing.
* Hallucinations are always a sign that an AI model is fundamentally broken.
* AI hallucinations are proof that AI models can’t be trusted in any context.
* AI hallucinations are a sign of sentience.
* AI hallucinations represent malicious intent or bias.
* AI hallucinations prove that AI models are fundamentally flawed and unreliable.
* AI hallucinations are just a side effect of generative AI and aren’t worth worrying about.

# 9. Falsehoods Regarding Model Transparency and Explainability

* Model transparency (e.g., open source) reduces hallucination risk.
* AI systems with explainability features don’t hallucinate.
* AI hallucinations can be fully corrected by retraining on more data.
* AI hallucinations are simply a matter of incorrect probabilities.
* AI hallucinations can be fully prevented by using larger datasets.
* AI systems with explainability features don’t hallucinate.
* AI hallucinations can be fully corrected by retraining on more data.
* AI systems with explainability features don’t hallucinate.

# 10. Falsehoods Regarding Hallucinations in AI Applications

* Hallucinations are purely a technical issue.
* AI hallucinations are mainly an issue with text generation, not in other domains.
* AI hallucinations are mainly an issue with text generation, not in other domains.
* AI hallucinations are solely a problem when AI models interact with humans.
* AI hallucinations are purely a technical issue.
* AI hallucinations are solely a problem when AI models interact with humans.
* AI hallucinations will always involve obvious factual errors.
* AI hallucinations occur mainly in low-resource languages.
* AI hallucinations will always involve obvious factual errors.
* AI hallucinations occur mainly in low-resource languages.
* AI hallucinations occur mainly in low-resource languages.
* AI hallucinations will always involve obvious factual errors.

# 11. Falsehoods Regarding AI Hallucinations and Creativity

* Hallucinations are evidence of creativity in AI.
* AI hallucinations indicate that the model is over-regularizing.
* AI hallucinations are a sign of sentience.
* Hallucinations are evidence of creativity in AI.
* AI hallucinations are proof that AI models can’t be trusted in any context.
* AI hallucinations represent malicious intent or bias.

# 12. Falsehoods Regarding AI Hallucinations and Edge Cases

* AI hallucinations only happen with edge-case scenarios.
* AI hallucinations are purely a matter of incorrect probabilities.
* AI hallucinations are only a problem with edge-case scenarios.
* AI hallucinations only happen with edge-case scenarios.
* AI hallucinations are purely a matter of incorrect probabilities.

# 13. Falsehoods Regarding Meta-Topics

* Only AI hallucinate, despite this list being based on all the times humans confidently belive wrong things.
* You've never met someone who hallucinated.
* OK, some humans hallucinate, but most of us don't.
* OK, most of us do, but you've never hallucinated.
* Recently.
* Today?
* While reading this list!?!?!?!?
* That there is some kind of prize for being the least-hallucinating kind of intelligence.
* That the methods used to deal with human hallucinations can't possibly work on AI.
* That the methods used to deal with human hallucinations are all we need for AI.
* That this list was written by a human.
* That the previous line means this section of this list was written by an AI.
